{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e940df89",
   "metadata": {},
   "source": [
    "## üîÅ Graph Representation Learning\n",
    "\n",
    "### ‚ùì What is Graph Representation Learning?\n",
    "\n",
    "In modern semi-supervised learning, **graph representation learning** (also called **graph embedding**) refers to learning **efficient and independent features** from graph nodes, with the aim of using them in machine learning tasks such as prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå How It Works:\n",
    "\n",
    "Each node `u` in the graph is mapped to a **low-dimensional vector** (embedding):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd2e1d",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/image1.png\" alt=\"Image\" width=\"600\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b5fa2",
   "metadata": {},
   "source": [
    "$$\n",
    "f: u \\to \\mathbb{R}^d\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5688c1",
   "metadata": {},
   "source": [
    "\n",
    "This resulting vector is known as the **embedding vector**.\n",
    "\n",
    "\n",
    "### üéØ Goal:\n",
    "\n",
    "The goal of these methods is to **embed nodes** into **low-dimensional vectors** in a way that **preserves their structural context** in the graph.  \n",
    "In other words, we want to **embed nodes into a hidden space** where the **geometric relationships** reflect the **original graph neighborhood structure**.\n",
    "\n",
    "\n",
    "### üß† Applications:\n",
    "\n",
    "- Node classification  \n",
    "- Link prediction  \n",
    "- Graph classification  \n",
    "- Anomaly detection  \n",
    "- Community detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb179e",
   "metadata": {},
   "source": [
    "## üìä Graph Embedding\n",
    "\n",
    "Graph embedding is a technique that can address the challenge of graph analysis in a **cost-effective** and **precise** manner.  \n",
    "This method converts the graph into a **vector-based representation** (typically in lower dimensions) based on its structure.\n",
    "\n",
    "\n",
    "\n",
    "### üñºÔ∏è Visual Explanation of Graph Embedding Types\n",
    "\n",
    "**(A)** Original Graph  \n",
    "Nodes are color-coded into three clusters:\n",
    "- Blue: A, B, C, D  \n",
    "- Yellow: E, F, G  \n",
    "- Red: H, I, J  \n",
    "\n",
    "\n",
    "\n",
    "**(B)** Node Embedding  \n",
    "Each **node** is embedded into a 2D space, preserving structural similarity.  \n",
    "Nodes from the same cluster are located close to each other.\n",
    "\n",
    "\n",
    "\n",
    "**(C)** Edge Embedding  \n",
    "Each **edge** is mapped to a point in 2D space.  \n",
    "The goal is to preserve the edge-level relationships.\n",
    "\n",
    "\n",
    "\n",
    "**(D)** Subgraph Embedding  \n",
    "Groups of nodes (subgraphs) are represented in a compact form, capturing local structures like communities.\n",
    "\n",
    "\n",
    "\n",
    "**(E)** Whole Graph Embedding  \n",
    "The entire graph is embedded into a single point in space ‚Äî useful for comparing whole graphs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba8134a",
   "metadata": {},
   "source": [
    "# How can we learn the embedding function *f*?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72a441",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/image2.png\" alt=\"Image\" width=\"600\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7039fb",
   "metadata": {},
   "source": [
    "## üî∑ Node Embedding: Encoding with Matrix Formulation\n",
    "\n",
    "We focus on **node embedding** based on the **encoder-decoder framework**.\n",
    "\n",
    "### ‚úÖ Encoder Function\n",
    "\n",
    "We define a function that maps each node $v \\in \\mathcal{V}$ to an embedding vector $\\mathbf{z}_v \\in \\mathbb{R}^d$:\n",
    "\n",
    "$\\text{ENC} : \\mathcal{V} \\rightarrow \\mathbb{R}^d,\\quad \\text{ENC}(v) = \\mathbf{z}_v$\n",
    "\n",
    "This function **encodes** the node $v$ into a low-dimensional vector representation.\n",
    "\n",
    "\n",
    "\n",
    "### üü® Embedding Matrix $Z$\n",
    "\n",
    "We can arrange all node embeddings in a matrix:\n",
    "\n",
    "$Z \\in \\mathbb{R}^{d \\times |\\mathcal{V}|}$\n",
    "\n",
    "Each **column** of $Z$ corresponds to the embedding of one node:\n",
    "\n",
    "$Z[:, v] = \\mathbf{z}_v$\n",
    "\n",
    "So we can write:\n",
    "\n",
    "$\\text{ENC}(v) = Z \\cdot \\mathbf{x}_v$\n",
    "\n",
    "where $\\mathbf{x}_v$ is a **one-hot vector** indicating the index of node $v$.\n",
    "\n",
    "\n",
    "\n",
    "### üìå Notes:\n",
    "\n",
    "- Each **column** of $Z$ represents the **embedding** for a node.\n",
    "- $\\mathbf{z}_v \\in \\mathbb{R}^d$: the latent representation of node $v$.\n",
    "- This formulation allows us to apply vector operations efficiently.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d73577c",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/image3.png\" alt=\"Image\" width=\"600\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cebbc6d",
   "metadata": {},
   "source": [
    "### üéØ Decoder Objective: Reconstructing the Relationship Between Nodes $u$ and $v$\n",
    "\n",
    "In graph representation learning, the **decoder** reconstructs information about the original graph using the node embeddings:\n",
    "\n",
    "$$\n",
    "\\text{DEC}: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^+\n",
    "$$\n",
    "\n",
    "The decoder receives the embeddings of nodes $u$ and $v$, and estimates their similarity:\n",
    "\n",
    "$$\n",
    "\\text{DEC}(\\text{ENC}(u), \\text{ENC}(v)) = \\text{DEC}(z_u, z_v) \\approx S[u, v]\n",
    "$$\n",
    "\n",
    "- Here, $S[u, v]$ is an entry in the **similarity matrix** $S$, which captures how similar or connected nodes $u$ and $v$ are in the original graph.\n",
    "\n",
    "\n",
    "\n",
    "### üí° A Simple Choice for $S$:\n",
    "\n",
    "We can use the adjacency matrix $A$ of the graph as the similarity matrix:\n",
    "\n",
    "$$\n",
    "S[u, v] \\triangleq A[u, v]\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "- $S[u, v] = 1$ if nodes $u$ and $v$ are connected  \n",
    "- $S[u, v] = 0$ otherwise\n",
    "\n",
    "### üîπ Common Neighbors\n",
    "\n",
    "To calculate the number of common neighbors between nodes $v_i$ and $v_j$, we can compute:\n",
    "\n",
    "$$\n",
    "S_{CN} = AA\n",
    "$$\n",
    "\n",
    "- For an **undirected graph**, the matrix $S_{CN}[i][j]$ shows the number of common neighbors between nodes $v_i$ and $v_j$.\n",
    "\n",
    "- For a **directed graph**, $S_{CN}[i][j]$ counts the number of nodes $v_k$ such that there are **paths from $v_j$ to $v_k$ and from $v_k$ to $v_i$** ‚Äî i.e., $v_j \\rightarrow v_k \\rightarrow v_i$.\n",
    "\n",
    "\n",
    "### üîÑ Pairwise Decoder\n",
    "\n",
    "A **pairwise decoder** $\\text{DEC}(z_u, z_v)$ predicts the **relationship or similarity** between nodes $u$ and $v$.  \n",
    "For example, it may estimate whether the two nodes are **neighbors** in the original graph.\n",
    "\n",
    "\n",
    "\n",
    "> ‚úÖ The goal is to **minimize reconstruction error** between the predicted similarity and the true similarity in $S$,  \n",
    "> thus optimizing both the **encoder** and the **decoder** functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7944bd89",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c0e2a",
   "metadata": {},
   "source": [
    "# üî∑ Node Representation Learning: Shallow Embedding\n",
    "\n",
    "- This is an **unsupervised** method for node representation learning.\n",
    "\n",
    "- We **do not use node labels**.\n",
    "- We **do not use node features**.\n",
    "\n",
    "- The goal is to **directly estimate an embedding vector for each node**, such that certain aspects of the graph structure are preserved.\n",
    "\n",
    "- These embeddings are **independent of any downstream prediction task**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc4646",
   "metadata": {},
   "source": [
    "## Shallow Embedding\n",
    "\n",
    "- $z_u$: the embedding of node $u$, which is the target of our learning process.\n",
    "\n",
    "- If $\\mathcal{D}$ is the set of training data pairs, the goal is to minimize the following loss function $\\mathcal{L}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{(u,v) \\in \\mathcal{D}} \\ell\\left( \\text{DEC}(z_u, z_v),\\ \\text{S}[u,v] \\right)\n",
    "$$\n",
    "\n",
    "where $\\ell: \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "- $\\ell$ is a loss function that measures the difference between the decoded similarity $\\text{DEC}(z_u, z_v)$ and the true similarity $S[u,v]$.\n",
    "\n",
    "- The loss function $\\ell$ can vary depending on how similarity and the decoder are defined. A common choice is the **mean squared error** for regression or classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603be01",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a263fca",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab792ca7",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3367b45",
   "metadata": {},
   "source": [
    "# Creating Node Representations with DeepWalk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7b5c9",
   "metadata": {},
   "source": [
    "\n",
    "# üìå Introducing Word2Vec\n",
    "\n",
    "The first step to comprehending the DeepWalk algorithm is to understand its major component: **Word2Vec**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What is Word2Vec?\n",
    "\n",
    "Word2Vec is one of the most influential deep-learning techniques in NLP.  \n",
    "Published in 2013 by **Tomas Mikolov et al.** at Google, it introduced a way to convert words into vectors ‚Äî called **embeddings** ‚Äî using large text datasets.\n",
    "\n",
    "These embeddings:\n",
    "\n",
    "- Allow computers to understand the **meaning of words** numerically.\n",
    "- Are useful in downstream tasks (like sentiment analysis or graph node classification).\n",
    "- Are a famous and patented example of successful ML architecture.\n",
    "\n",
    "\n",
    "## üìä Example Embeddings\n",
    "\n",
    "Here are a few example word vectors:\n",
    "\n",
    "```\n",
    "vec(king)   = [‚àí2.1, 4.1, 0.6]  \n",
    "vec(queen)  = [‚àí1.9, 2.6, 1.5]  \n",
    "vec(man)    = [3.0, ‚àí1.1, ‚àí2.0]  \n",
    "vec(woman)  = [2.8, ‚àí2.6, ‚àí1.1]\n",
    "```\n",
    "\n",
    "These are simplified 3-dimensional representations, but real embeddings are often 100‚Äì300 dimensions.\n",
    "\n",
    "\n",
    "## üßÆ Similarity by Euclidean Distance\n",
    "\n",
    "Let‚Äôs compare the Euclidean distances between words:\n",
    "\n",
    "- Distance between **king** and **queen**: $4.37$\n",
    "- Distance between **king** and **woman**: $8.47$\n",
    "\n",
    "This tells us:\n",
    "\n",
    "‚û° $vec(king)$ is **closer** to $vec(queen)$ than it is to $vec(woman)$.\n",
    "\n",
    "\n",
    "\n",
    "## üîÅ Cosine Similarity (Angle-Based Comparison)\n",
    "\n",
    "Instead of using distances, **cosine similarity** is often used.  \n",
    "It compares **angles**, not magnitudes ‚Äî which makes it better when lengths differ.\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(\\vec{A}, \\vec{B}) = \\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\cdot \\|\\vec{B}\\|}\n",
    "$$\n",
    "\n",
    "- $\\vec{A} \\cdot \\vec{B}$ = dot product  \n",
    "- $\\|\\vec{A}\\|$ = length (norm) of vector A\n",
    "\n",
    "\n",
    "\n",
    "## üí° Vector Arithmetic: Word Analogies\n",
    "\n",
    "One surprising ability of Word2Vec is solving analogies using simple vector math.\n",
    "\n",
    "Famous example:\n",
    "\n",
    "> \"**man** is to **woman** as **king** is to ___?\"\n",
    "\n",
    "This is calculated as:\n",
    "\n",
    "$$\n",
    "\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}\n",
    "$$\n",
    "\n",
    "This relationship doesn‚Äôt always hold exactly, but it works surprisingly well in many cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98c84e",
   "metadata": {},
   "source": [
    "# CBOW vs Skip-gram (Word2Vec)\n",
    "\n",
    "## üß† Core Concept\n",
    "Both **CBOW** and **Skip-gram** are models used in Word2Vec to learn word embeddings ‚Äî dense vector representations of words ‚Äî based on their context in a sentence.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò CBOW (Continuous Bag-of-Words)\n",
    "This is trained to predict a word using its\n",
    "surrounding context (words coming before and after the target word). The order of context\n",
    "words does not matter since their embeddings are summed in the model. The authors claim to\n",
    "obtain better results using four words before and after the one that is predicted.\n",
    "\n",
    "### ‚úÖ Goal:\n",
    "Predict the **center word** given its **context words**.\n",
    "\n",
    "### üìå Example:\n",
    "For the sentence:  \n",
    "`The cat sits on the mat`\n",
    "\n",
    "CBOW input and output would be:  \n",
    "**Input (Context):** `The, cat, on, the`  \n",
    "**Output (Target):** `sits`\n",
    "\n",
    "\n",
    "## üìó Skip-gram\n",
    "Here, we feed a single word to the model and try to predict\n",
    "the words around it. Increasing the range of context words leads to better embeddings but also\n",
    "increases the training time.\n",
    "\n",
    "\n",
    "### ‚úÖ Goal:\n",
    "Predict the **context words** given the **center word**.\n",
    "\n",
    "### üìå Example:\n",
    "For the sentence:  \n",
    "`The cat sits on the mat`\n",
    "\n",
    "Skip-gram input and output would be:  \n",
    "**Input (Center):** `sits`  \n",
    "**Output (Context):** `The, cat, on, the`\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/image4.jpg\" alt=\"Image\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "## üìä Comparison Table\n",
    "\n",
    "| Feature         | CBOW                          | Skip-gram                        |\n",
    "|----------------|-------------------------------|----------------------------------|\n",
    "| Input           | Context words                 | Center word                      |\n",
    "| Output          | Center word                   | Context words                    |\n",
    "| Speed           | Faster to train               | Slower but more accurate         |\n",
    "| Good for        | Frequent words                | Rare words                       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614d731",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25458154",
   "metadata": {},
   "source": [
    "\n",
    "## üìå Creating Skip-grams\n",
    "\n",
    "For now, we will focus on the **skip-gram model** since it is the architecture used by **DeepWalk**.\n",
    "\n",
    "Skip-grams are implemented as **pairs of words** with the following structure:\n",
    "\n",
    "```\n",
    "(target word, context word)\n",
    "```\n",
    "\n",
    "- `target word`: the input word to the model.\n",
    "- `context word`: the word the model tries to predict (surrounding words).\n",
    "\n",
    "\n",
    "\n",
    "### üîß Parameter: `context size`\n",
    "\n",
    "The number of skip-gram pairs generated for a given target word depends on a parameter called **context size**, which defines how many words before and after the target word are considered context.\n",
    "\n",
    "\n",
    "\n",
    "### üìä Example \n",
    "\n",
    "Let‚Äôs take the sentence:  \n",
    "**\"the train was late\"**\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/image5.jpg\" alt=\"Image\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "### üß† Practical Usage\n",
    "\n",
    "- The same idea applies to a **corpus** of text, not just a single sentence.\n",
    "- We **store all context words** for the same target word in a **list** to save memory.\n",
    "- The next example will apply this to an entire paragraph, using:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7e7af",
   "metadata": {},
   "source": [
    "### In the following example, we create skip-grams for an entire paragraph stored in the text variable. We set the CONTEXT_SIZE variable to 2, which means we will look at the two words before and after our target word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e0e8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5f5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b461ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "\n",
    "text = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc eu sem \n",
    "scelerisque, dictum eros aliquam, accumsan quam. Pellentesque tempus, lorem ut \n",
    "semper fermentum, ante turpis accumsan ex, sit amet ultricies tortor erat quis \n",
    "nulla. Nunc consectetur ligula sit amet purus porttitor, vel tempus tortor \n",
    "scelerisque. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices \n",
    "posuere cubilia curae; Quisque suscipit ligula nec faucibus accumsan. Duis \n",
    "vulputate massa sit amet viverra hendrerit. Integer maximus quis sapien id \n",
    "convallis. Donec elementum placerat ex laoreet gravida. Praesent quis enim \n",
    "facilisis, bibendum est nec, pharetra ex. Etiam pharetra congue justo, eget \n",
    "imperdiet diam varius non. Mauris dolor lectus, interdum in laoreet quis, \n",
    "faucibus vitae velit. Donec lacinia dui eget maximus cursus. Class aptent taciti\n",
    "sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Vivamus\n",
    "tincidunt velit eget nisi ornare convallis. Pellentesque habitant morbi \n",
    "tristique senectus et netus et malesuada fames ac turpis egestas. Donec \n",
    "tristique ultrices tortor at accumsan.\n",
    "\"\"\".split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fec29",
   "metadata": {},
   "source": [
    "Next, we create the skip-grams thanks to a simple for loop to consider every word in text.\n",
    "A list comprehension generates the context words, stored in the skipgrams list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f99a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create skipgrams\n",
    "skipgrams = []\n",
    "for i in range(CONTEXT_SIZE, len(text) - CONTEXT_SIZE):\n",
    "    array = [text[j] for j in np.arange(i - CONTEXT_SIZE, i + CONTEXT_SIZE + 1) if j != i]\n",
    "    skipgrams.append((text[i], array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5c0b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dolor', ['Lorem', 'ipsum', 'sit', 'amet,']), ('sit', ['ipsum', 'dolor', 'amet,', 'consectetur'])]\n"
     ]
    }
   ],
   "source": [
    "print(skipgrams[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d1346",
   "metadata": {},
   "source": [
    "These two target words, with their corresponding context, work to show what the inputs to Word2Vec\n",
    "look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095f5e1",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66355c7",
   "metadata": {},
   "source": [
    "# üß† The Skip-Gram Model\n",
    "\n",
    "The goal of Word2Vec is to produce **high-quality word embeddings**.  \n",
    "To learn these embeddings, the training task of the skip-gram model consists of **predicting the correct context words given a target word**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Objective\n",
    "\n",
    "Imagine that we have a sequence of $N$ words:\n",
    "\n",
    "$$\n",
    "w_1, w_2, \\dots, w_N\n",
    "$$\n",
    "\n",
    "The probability of seeing the word $w_2$ given the word $w_1$ is written as:\n",
    "\n",
    "$$\n",
    "p(w_2 \\mid w_1)\n",
    "$$\n",
    "\n",
    "The goal is to **maximize the sum of every probability of seeing a context word given a target word** in the entire text:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{\\substack{-C \\le j \\le C \\\\ j \\neq 0}} \\log p(w_{n+j} \\mid w_n)\n",
    "$$\n",
    "\n",
    "where $C$ is the size of the context window.\n",
    "\n",
    "\n",
    "\n",
    "## üì¶ Why Use Log Probability?\n",
    "\n",
    "> **Note:**  \n",
    "> We use log probabilities for two main reasons:\n",
    "\n",
    "1. **Efficiency:**  \n",
    "   $$\n",
    "   \\log(A \\times B) = \\log(A) + \\log(B)\n",
    "   $$\n",
    "   So multiplication operations turn into addition, which is computationally cheaper.\n",
    "\n",
    "2. **Numerical Stability:**  \n",
    "   Computers struggle with **very small numbers**, such as $3.14 \\times 10^{-128}$.\n",
    "   Using logs:\n",
    "   $$\n",
    "   \\log(3.14 \\times 10^{-128}) = -127.5\n",
    "   $$\n",
    "   avoids underflow issues.\n",
    "\n",
    "\n",
    "## üßÆ Softmax-Based Probability\n",
    "\n",
    "The skip-gram model uses **softmax** to calculate the probability of a context word $w_c$ given a target word $w_t$.\n",
    "\n",
    "Given:\n",
    "\n",
    "- $h_t$: embedding of target word  \n",
    "- $h_c$: embedding of context word  \n",
    "- $|V|$: vocabulary size  \n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "p(w_c \\mid w_t) \n",
    "= \\frac{\\exp(h_c^\\top h_t)}\n",
    "{\\sum_{i=1}^{|V|} \\exp(h_i^\\top h_t)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Vocabulary Size in Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c47510",
   "metadata": {},
   "source": [
    "The vocabulary corresponds to the list of unique words the\n",
    "model tries to predict. We can obtain this list using the set data structure to remove duplicate words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87286cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary = 121\n"
     ]
    }
   ],
   "source": [
    "vocab = set(text)\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(f\"Length of vocabulary = {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e52bcc",
   "metadata": {},
   "source": [
    "### Word2Vec Skip-gram Model Explanation\n",
    "\n",
    "Now that we have the size of our vocabulary, there is one more parameter we need to define: **N**, the dimensionality of the word vectors. Typically, this value is set between 100 and 1,000. In this example, we will set it to 10 because of the limited size of our dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### The Skip-gram Model Architecture\n",
    "\n",
    "The skip-gram model is composed of only two layers:\n",
    "\n",
    "- **A projection layer** with a weight matrix \\( W_{\\text{embed}} \\), which takes a one-hot encoded word vector as an input and returns the corresponding N-dim word embedding. It acts as a simple lookup table that stores embeddings of a predefined dimensionality.\n",
    "\n",
    "- **A fully connected layer** with a weight matrix \\( W_{\\text{output}} \\), which takes a word embedding as input and outputs \\( |\\mathcal{V}| \\)-dim logits. A softmax function is applied to these predictions to transform logits into probabilities.\n",
    "\n",
    "> **Note:**  \n",
    "> There is no activation function: Word2Vec is a linear classifier that models a linear relationship between words.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Details\n",
    "\n",
    "Let‚Äôs call \\( \\mathbf{x} \\) the one-hot encoded word vector (the **input**). The corresponding word embedding can be calculated as a simple projection:\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = W_{\\text{embed}}^T \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Using the skip-gram model, we can rewrite the probability of context word \\( w_c \\) given target word \\( w_t \\) as:\n",
    "\n",
    "$$\n",
    "p(w_c | w_t) = \\frac{\\exp(W_{\\text{output}} \\cdot \\mathbf{h})}{\\sum_{i=1}^{|\\mathcal{V}|} \\exp(W_{\\text{output}(i)} \\cdot \\mathbf{h})}\n",
    "$$\n",
    "\n",
    "The skip-gram model outputs a \\( |\\mathcal{V}| \\)-dimensional vector, which is the conditional probability of every word in the vocabulary:\n",
    "\n",
    "$$\n",
    "\\text{word2vec}(w_t) = \n",
    "\\begin{bmatrix}\n",
    "p(w_1 | w_t) \\\\\n",
    "p(w_2 | w_t) \\\\\n",
    "\\vdots \\\\\n",
    "p(w_{|\\mathcal{V}|} | w_t)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "During training, these probabilities are compared to the correct one-hot encoded target word vectors.  \n",
    "The difference between these values (calculated by a loss function such as the cross-entropy loss) is backpropagated through the network to update the weights and obtain better predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce88ff",
   "metadata": {},
   "source": [
    "The entire Word2Vec architecture is summarized in the following diagram, with both matrices and\n",
    "the final softmax layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d522243",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/image6.jpg\" alt=\"Image\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f1e3a",
   "metadata": {},
   "source": [
    "### We can implement this model using the gensim library, which is also used in the official implementation of DeepWalk. We can then build the vocabulary and train our model based on the previous text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b581e",
   "metadata": {},
   "source": [
    "Let‚Äôs begin by installing gensim and importing the Word2Vec class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d1bffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a608dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd306df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    [text], \n",
    "    sg=1,   # Skip-gram\n",
    "    vector_size=10,\n",
    "    min_count=0,\n",
    "    window=2,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42049a2d",
   "metadata": {},
   "source": [
    "It‚Äôs a good idea to check the shape of our first weight matrix. It should correspond to the\n",
    "vocabulary size and the word embeddings‚Äô dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65c1dd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W_embed: (121, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of W_embed: {model.wv.vectors.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30dc1f2",
   "metadata": {},
   "source": [
    "Next, we train the model for 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978a7291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(718, 1560)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train([text], total_examples=model.corpus_count,\n",
    "epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc1ef79",
   "metadata": {},
   "source": [
    "Finally, we can print a word embedding to see what the result of this training looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1852db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding =\n",
      "[-0.08092759  0.05650158  0.03220123 -0.01081672 -0.01091846  0.07142022\n",
      " -0.08430826  0.03745409 -0.06069671 -0.08211951]\n"
     ]
    }
   ],
   "source": [
    "print('Word embedding =')\n",
    "print(model.wv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1334c",
   "metadata": {},
   "source": [
    "While this approach works well with small vocabularies, the computational cost of applying a full\n",
    "softmax function to millions of words (the vocabulary size ) is too costly in most cases. This has been\n",
    "a limiting factor in developing accurate language models for a long time. Fortunately for us, other\n",
    "approaches have been designed to solve this issue.\n",
    "Word2Vec (and DeepWalk) implements one of these techniques, called H-Softmax. Instead of a flat\n",
    "softmax that directly calculates the probability of every word, this technique uses a binary tree structure\n",
    "where leaves are words. Even more interestingly, a Huffman tree can be used, where infrequent words\n",
    "are stored at deeper levels than common words. In most cases, this dramatically speeds up the word\n",
    "prediction by a factor of at least 50.\n",
    "H-Softmax can be activated in gensim using hs=1.\n",
    "This was the most difficult part of the DeepWalk architecture. But before we can implement it, we\n",
    "need one more component: how to create our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e10239",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a709b",
   "metadata": {},
   "source": [
    "## üîç DeepWalk and Random Walks ‚Äî Key Concepts Explained\n",
    "\n",
    "DeepWalk, introduced by Perozzi et al. in 2014, is a graph embedding technique inspired by Word2Vec from NLP. It quickly gained popularity because of its simplicity, strong baseline performance, and ability to scale to large graphs.\n",
    "\n",
    "### ‚úÖ Objective of DeepWalk\n",
    "The primary goal of DeepWalk is to learn **high-quality feature representations of graph nodes** in an **unsupervised** manner. It aims to embed nodes in a continuous vector space where semantically or structurally similar nodes are close together.\n",
    "\n",
    "\n",
    "\n",
    "### üîÅ Similarity to Word2Vec\n",
    "\n",
    "DeepWalk is heavily inspired by the **Word2Vec** model. Here's the analogy:\n",
    "\n",
    "| Concept          | In Word2Vec (NLP)       | In DeepWalk (Graph)     |\n",
    "|------------------|--------------------------|---------------------------|\n",
    "| Training data    | Sentences (word sequences) | Random walks (node sequences) |\n",
    "| Unit of meaning  | Words                    | Nodes                      |\n",
    "| Model            | Word2Vec (Skip-gram)     | Word2Vec (Skip-gram)       |\n",
    "| Goal             | Learn word embeddings    | Learn node embeddings      |\n",
    "\n",
    "---\n",
    "\n",
    "### üö∂‚Äç‚ôÇÔ∏è Random Walks as \"Sentences\"\n",
    "\n",
    "- Random walks are used to create sequences of nodes.\n",
    "- These node sequences act like **sentences**, where each node is treated like a **word**.\n",
    "- DeepWalk performs multiple random walks starting from each node to generate training data.\n",
    "\n",
    "The following diagram illustrates the connection between sentences and graphs:\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/image7.jpg\" alt=\"Image\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "### üîÅ How Random Walks Work\n",
    "\n",
    "- Starting from a node, the walker randomly chooses a neighboring node at each step.\n",
    "- This process is repeated to generate multiple sequences per node.\n",
    "- **Nodes can appear multiple times** in the same sequence due to cycles or revisiting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0781e0",
   "metadata": {},
   "source": [
    "This idea is at the core of the DeepWalk algorithm: when nodes are close to each other, we want to\n",
    "obtain high similarity scores. On the contrary, we want low scores when they are farther apart.\n",
    "Let‚Äôs implement a random walk function using a networkx graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3ed7d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
